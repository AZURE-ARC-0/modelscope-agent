{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf9e03b-a513-4b81-a75e-243f38a3c84a",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3851d799-7162-4e73-acab-3c13cb1e43bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1ede08-857f-4592-9093-d7e5a37ce245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/workspace/code/ModelScopeGPT/agent/demo', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/tinycudann-1.6-py3.8-linux-x86_64.egg', '../', '../', '../']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42f062-5664-44f8-82b7-6995a4f512a0",
   "metadata": {},
   "source": [
    "## Load cfg and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f06ad1-34c5-4032-9254-aab88d7fe19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../config/cfg_model.json\n",
      "../config/cfg_tool.json\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../config/.env', override=True)\n",
    "\n",
    "import os\n",
    "\n",
    "from modelscope.utils.config import Config\n",
    "\n",
    "model_cfg_file = os.getenv('LLM_CONFIG_FILE')\n",
    "tool_cfg_file = os.getenv('TOOL_CONFIG_FILE') \n",
    "\n",
    "print(model_cfg_file)\n",
    "print(tool_cfg_file)\n",
    "\n",
    "model_cfg = Config.from_file(model_cfg_file)\n",
    "tool_cfg = Config.from_file(tool_cfg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a0805a-53d9-43e0-a63d-b9dd801039fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'Hello World!\\n1.txt\\ndemo_chatgpt.ipynb\\ndemo.ipynb\\ndemo_local_llm.ipynb\\nnew_directory\\ntmp\\ntool_agent_finetune\\n'}\n",
      "{'terminal': <modelscope_agent.tools.plugin_tool.LangchainTool object at 0x7fa5b48ed730>, 'read_file': <modelscope_agent.tools.plugin_tool.LangchainTool object at 0x7fa5b48ed910>}\n"
     ]
    }
   ],
   "source": [
    "# tool检验\n",
    "\n",
    "from modelscope_agent.tools import LangchainTool\n",
    "from langchain.tools import ShellTool, ReadFileTool\n",
    "\n",
    "\n",
    "shell_tool = LangchainTool(ShellTool())\n",
    "read_tool = LangchainTool(ReadFileTool())\n",
    "\n",
    "\n",
    "additional_tool_list = {\n",
    "    shell_tool.name: shell_tool,\n",
    "    read_tool.name: read_tool\n",
    "}\n",
    "\n",
    "print(shell_tool(commands=[\"echo 'Hello World!'\", \"ls\"]))\n",
    "print(additional_tool_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbbee3d8-067b-405d-b630-2a74d4e78b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope_agent.agent import AgentExecutor\n",
    "from modelscope_agent.llm.openai import OpenAi\n",
    "from modelscope_agent.prompt import DEFAULT_CHATGPT_PROMPT_TEMPLATE, MSPromptGenerator\n",
    "\n",
    "\n",
    "\n",
    "prompt_generator = MSPromptGenerator(DEFAULT_CHATGPT_PROMPT_TEMPLATE)\n",
    "llm = OpenAi(model_cfg)\n",
    "agent = AgentExecutor(llm, tool_cfg, additional_tool_list=additional_tool_list, prompt_generator=prompt_generator)\n",
    "\n",
    "available_tool_list = [\n",
    "    'terminal',\n",
    "    'read_file'\n",
    "]\n",
    "\n",
    "agent.set_available_tools(available_tool_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a8324-e589-47a1-b948-d2a89a75da53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tool检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c680dad9-3226-4972-9bd1-4b25a9583a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**************************************************round 1**************************************************"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "api_name": "terminal",
       "parameters": {
        "commands": "ls"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sure, I can help you with that. Please use the following command:\n",
       "\n",
       "'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|action: terminal, action_args: {'commands': 'ls'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "**************************************************round 2**************************************************"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "The command 'ls' has been executed and the following files and directories have been found: 1.txt, demo_chatgpt.ipynb, demo.ipynb, demo_local_llm.ipynb, new_directory, tmp, tool_agent_finetune."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|action: None, action_args: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'result': '1.txt\\ndemo_chatgpt.ipynb\\ndemo.ipynb\\ndemo_local_llm.ipynb\\nnew_directory\\ntmp\\ntool_agent_finetune\\n'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.reset()\n",
    "agent.run('use tool to execute command \\'ls\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58b1a8d5-8bcc-4f04-b242-2b2a314320b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**************************************************round 1**************************************************"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "api_name": "read_file",
       "parameters": {
        "file_path": "1.txt"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sure, I can help you with that. To read the file content of 1.txt, we can use the \"read_file\" tool. Here is the command: \n",
       "\n",
       "'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|action: read_file, action_args: {'file_path': '1.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "**************************************************round 2**************************************************"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "The content of the 1.txt file is \"hello!\"."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|action: None, action_args: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'result': 'hello!'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.reset()\n",
    "agent.run('please help read the file content of 1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3aab687-9c54-435b-930c-74c60d88f253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "**************************************************round 1**************************************************"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "api_name": "hf_text_translation",
       "parameters": {
        "src_lang": "English",
        "text": "hello",
        "tgt_lang": "French"
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sure, I can help you with that. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|action: hf_text_translation, action_args: {'text': 'hello', 'src_lang': 'English', 'tgt_lang': 'French'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d1c5a31b9c41c185df5f0ce679b1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa71afaa611c4bc898f18657174a7471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e7886330264714b0d5b232e215e50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ed343c3244aa889467039e4f1cf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee81acb28fa54400b1e423e8b015ab43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c237779bf5de46429d06cd79edce55c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6193d9bb67cd4695bd495ea24ef2d461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.backends' has no attribute 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 17\u001B[0m\n\u001B[1;32m     13\u001B[0m available_tool_list \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhf_text_translation\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     15\u001B[0m ]\n\u001B[1;32m     16\u001B[0m agent\u001B[38;5;241m.\u001B[39mset_available_tools(available_tool_list)\n\u001B[0;32m---> 17\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtranslate the following English into French: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhello\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/workspace/code/ModelScopeGPT/agent/demo/../modelscope_agent/agent.py:122\u001B[0m, in \u001B[0;36mAgentExcutor.run\u001B[0;34m(self, task, remote)\u001B[0m\n\u001B[1;32m    120\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_exec_result(exec_result)\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 122\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     exec_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown action: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maction\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/mnt/workspace/code/ModelScopeGPT/agent/demo/../modelscope_agent/agent.py:115\u001B[0m, in \u001B[0;36mAgentExcutor.run\u001B[0;34m(self, task, remote)\u001B[0m\n\u001B[1;32m    113\u001B[0m tool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtool_list[action]\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 115\u001B[0m     exec_result \u001B[38;5;241m=\u001B[39m \u001B[43mtool\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43maction_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremote\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# print(f'|exec_result: {exec_result}')\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \n\u001B[1;32m    118\u001B[0m     \u001B[38;5;66;03m# parse exec result and store result to agent state\u001B[39;00m\n\u001B[1;32m    119\u001B[0m     final_res\u001B[38;5;241m.\u001B[39mappend(exec_result)\n",
      "File \u001B[0;32m/mnt/workspace/code/ModelScopeGPT/agent/demo/../modelscope_agent/tools/tool.py:60\u001B[0m, in \u001B[0;36mTool.__call__\u001B[0;34m(self, remote, *args, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_remote_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_local_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/workspace/code/ModelScopeGPT/agent/demo/../modelscope_agent/tools/hf_text_translation_tool.py:31\u001B[0m, in \u001B[0;36mHFTextTranslationTool._local_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_local_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m}\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tools/base.py:532\u001B[0m, in \u001B[0;36mPipelineTool.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_initialized:\n\u001B[0;32m--> 532\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    534\u001B[0m     encoded_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    535\u001B[0m     encoded_inputs \u001B[38;5;241m=\u001B[39m send_to_device(encoded_inputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tools/base.py:506\u001B[0m, in \u001B[0;36mPipelineTool.setup\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mhf_device_map\u001B[38;5;241m.\u001B[39mvalues())[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 506\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m \u001B[43mget_default_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tools/base.py:573\u001B[0m, in \u001B[0;36mget_default_device\u001B[0;34m()\u001B[0m\n\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available():\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install torch in order to use this tool.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmps\u001B[49m\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mmps\u001B[38;5;241m.\u001B[39mis_built():\n\u001B[1;32m    574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch.backends' has no attribute 'mps'"
     ]
    }
   ],
   "source": [
    "from modelscope_agent.tools import HFTool\n",
    "\n",
    "\n",
    "hf_text_translation_tool = HFTool()\n",
    "\n",
    "additional_tool_list = {\n",
    "    'hf_text_translation': hf_text_translation_tool\n",
    "}\n",
    "\n",
    "agent = AgentExecutor(llm, tool_cfg, additional_tool_list=additional_tool_list, prompt_generator=prompt_generator)\n",
    "agent.reset()\n",
    "\n",
    "available_tool_list = [\n",
    "    'hf_text_translation'\n",
    "]\n",
    "agent.set_available_tools(available_tool_list)\n",
    "agent.run(\"translate the following English into French: 'hello' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84e1bb-e643-4fcc-a416-0f8088dc6abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
